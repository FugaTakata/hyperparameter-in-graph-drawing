{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuga_takata/dev/vdslab-project/hyperparameter_optimization/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "\n",
    "import json\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from egraph import Graph, Coordinates, Rng, SparseSgd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_WEIGHT = 30\n",
    "K_FROM = 1\n",
    "K_TO = 20\n",
    "\n",
    "LARGE_DATASET_NAMES = [\n",
    "    '3elt.json',\n",
    "    '1138_bus.json',\n",
    "    'dwt_1005.json',\n",
    "    'dwt_2680.json',\n",
    "    'poli.json',\n",
    "    'qh882.json',\n",
    "    'USpowerGrid.json',\n",
    "]\n",
    "SMALL_DATASET_NAMES = [\n",
    "    'bull.json',\n",
    "    'chvatal.json',\n",
    "    'cubical.json',\n",
    "    'davis_southern_women.json',\n",
    "    'desargues.json',\n",
    "    'diamond.json',\n",
    "    'dodecahedral.json',\n",
    "    'florentine_families.json',\n",
    "    'frucht.json',\n",
    "    'heawood.json',\n",
    "    'hoffman_singleton.json',\n",
    "    'house_x.json',\n",
    "    'house.json',\n",
    "    'icosahedral.json',\n",
    "    'karate_club.json',\n",
    "    'krackhardt_kite.json',\n",
    "    'les_miserables.json',\n",
    "    'moebius_kantor.json',\n",
    "    'octahedral.json',\n",
    "    'pappus.json',\n",
    "    'petersen.json',\n",
    "    'sedgewick_maze.json',\n",
    "    'tutte.json',\n",
    "]\n",
    "\n",
    "DATASET_NAMES = SMALL_DATASET_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフの生成・読み込み\n",
    "\n",
    "def generate_graph_from_nx_graph(nx_graph):\n",
    "    graph = Graph()\n",
    "\n",
    "    indices = {}\n",
    "    for u in nx_graph.nodes:\n",
    "        indices[u] = graph.add_node(u)\n",
    "    for u, v in nx_graph.edges:\n",
    "        graph.add_edge(indices[u], indices[v], (u, v))\n",
    "\n",
    "    return graph, indices\n",
    "\n",
    "\n",
    "def graph_preprocessing(nx_graph):\n",
    "    nx_graph = nx.Graph(nx_graph)\n",
    "\n",
    "    # グラフを無向グラフに\n",
    "    nx_graph = nx_graph.to_undirected()\n",
    "\n",
    "    # エッジのループを除去\n",
    "    nx_graph.remove_edges_from(list(nx.selfloop_edges(nx_graph)))\n",
    "\n",
    "    # 最大連結成分を用いる\n",
    "    largest_cc = max(nx.connected_components(nx_graph), key=len)\n",
    "    largest_cc_graph = nx_graph.subgraph(largest_cc)\n",
    "\n",
    "    new_graph = nx.Graph()\n",
    "    nodes = [str(node_id) for node_id in largest_cc_graph.nodes]\n",
    "    new_graph.add_nodes_from(nodes)\n",
    "\n",
    "    # エッジにidと重みを追加\n",
    "    for i, edge in enumerate(largest_cc_graph.edges):\n",
    "        new_graph.add_edge(str(edge[0]), str(edge[1]), weight=EDGE_WEIGHT, id=str(i))\n",
    "        # weighted_edges.append((str(edge[0]), str(edge[1]), EDGE_WEIGHT))\n",
    "\n",
    "    return new_graph\n",
    "\n",
    "\n",
    "nx_graphs = []\n",
    "dir = 'lib/egraph-rs/js/dataset/'\n",
    "for filename in DATASET_NAMES:\n",
    "    with open(dir + filename) as f:\n",
    "        graph_data = json.load(f)\n",
    "\n",
    "    nx_graph = nx.node_link_graph(graph_data)\n",
    "    nx_processed_graph = graph_preprocessing(nx_graph)\n",
    "    nx_graphs.append(nx_processed_graph)\n",
    "\n",
    "# 追加のnx.scale_free_graph\n",
    "for i in range(1, 3 + 1):\n",
    "    n = i * 1000\n",
    "    nx_graphs.append(graph_preprocessing(nx.scale_free_graph(n)))\n",
    "\n",
    "    DATASET_NAMES.append(f'nx_scale_free_{n}')\n",
    "\n",
    "graph_data = []\n",
    "for nx_graph in nx_graphs:\n",
    "    graph, indices = generate_graph_from_nx_graph(nx_graph)\n",
    "    graph_data.append((graph, indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape graph生成\n",
    "\n",
    "\n",
    "def generate_k_nearest_graph(pos, K, distances=None):\n",
    "    k_nearest_graph = nx.Graph()\n",
    "\n",
    "    nodes = [node_id for node_id in pos]\n",
    "    k_nearest_graph.add_nodes_from(nodes)\n",
    "\n",
    "    if distances is None:\n",
    "        distances = {}\n",
    "        for source in pos:\n",
    "            distances[source] = []\n",
    "            source_pos = np.array(pos[source])\n",
    "            for target in pos:\n",
    "                if target == source:\n",
    "                    continue\n",
    "                target_pos = np.array(pos[target])\n",
    "                distance = np.linalg.norm(source_pos - target_pos)\n",
    "\n",
    "                distances[source].append({'id': target, 'distance': distance})\n",
    "\n",
    "        for source in pos:\n",
    "            distances[source] = sorted(\n",
    "                distances[source], key=lambda x: x['distance'])\n",
    "\n",
    "    weighted_edges = []\n",
    "    for source in pos:\n",
    "        for distance in distances[source][:K]:\n",
    "            weighted_edges.append((source, distance['id'], EDGE_WEIGHT))\n",
    "    k_nearest_graph.add_weighted_edges_from(weighted_edges)\n",
    "\n",
    "    return k_nearest_graph, distances\n",
    "\n",
    "\n",
    "def generate_degree_based_k_nearest(nx_graph, pos, distances=None, degrees=None):\n",
    "    k_nearest_graph = nx.Graph()\n",
    "\n",
    "    nodes = [node_id for node_id in pos]\n",
    "    k_nearest_graph.add_nodes_from(nodes)\n",
    "\n",
    "    if distances is None:\n",
    "        distances = {}\n",
    "        for source in pos:\n",
    "            distances[source] = []\n",
    "            source_pos = np.array(pos[source])\n",
    "            for target in pos:\n",
    "                if target == source:\n",
    "                    continue\n",
    "                target_pos = np.array(pos[target])\n",
    "                distance = np.linalg.norm(source_pos - target_pos)\n",
    "\n",
    "                distances[source].append({'id': target, 'distance': distance})\n",
    "\n",
    "        for source in pos:\n",
    "            distances[source] = sorted(\n",
    "                distances[source], key=lambda x: x['distance'])\n",
    "\n",
    "    if degrees is None:\n",
    "        degrees = {}\n",
    "        for node_id in pos:\n",
    "            degrees[node_id] = nx_graph.degree[node_id]\n",
    "\n",
    "    weighted_edges = []\n",
    "    for source in pos:\n",
    "        for distance in distances[source][:degrees[node_id]]:\n",
    "            weighted_edges.append((source, distance['id'], EDGE_WEIGHT))\n",
    "    k_nearest_graph.add_weighted_edges_from(weighted_edges)\n",
    "\n",
    "    return k_nearest_graph, distances, degrees\n",
    "\n",
    "\n",
    "def generate_delaunay_triangulation_graph(pos):\n",
    "    index_id_map = {}\n",
    "    pos_array = []\n",
    "    for index, node_id in enumerate(pos):\n",
    "        positions = pos[node_id]\n",
    "        position = [positions[0], positions[1]]\n",
    "        pos_array.append(position)\n",
    "        index_id_map[index] = node_id\n",
    "\n",
    "    pos_ndarray = np.array(pos_array)\n",
    "    delaunay = Delaunay(pos_ndarray)\n",
    "\n",
    "    delaunay_triangulation_graph = nx.Graph()\n",
    "\n",
    "    nodes = [node_id for node_id in pos]\n",
    "    delaunay_triangulation_graph.add_nodes_from(nodes)\n",
    "\n",
    "    weighted_edges = []\n",
    "    for n in delaunay.simplices:\n",
    "        n0 = n[0]\n",
    "        n1 = n[1]\n",
    "        n2 = n[2]\n",
    "        weighted_edges.append(\n",
    "            (index_id_map[n0], index_id_map[n1], EDGE_WEIGHT))\n",
    "        weighted_edges.append(\n",
    "            (index_id_map[n0], index_id_map[n2], EDGE_WEIGHT))\n",
    "        weighted_edges.append(\n",
    "            (index_id_map[n1], index_id_map[n2], EDGE_WEIGHT))\n",
    "    delaunay_triangulation_graph.add_weighted_edges_from(weighted_edges)\n",
    "\n",
    "    return delaunay_triangulation_graph\n",
    "\n",
    "\n",
    "def generate_mst_graph(pos):\n",
    "    index_id_map = {}\n",
    "    pos_array = []\n",
    "    for index, node_id in enumerate(pos):\n",
    "        positions = pos[node_id]\n",
    "        position = [positions[0], positions[1]]\n",
    "        pos_array.append(position)\n",
    "        index_id_map[index] = node_id\n",
    "\n",
    "    dists = []\n",
    "    for source_pos in pos_array:\n",
    "        source_pos_ndarray = np.array(source_pos)\n",
    "        tmp = []\n",
    "        for target_pos in pos_array:\n",
    "            target_pos_ndarray = np.array(target_pos)\n",
    "            tmp.append(np.linalg.norm(source_pos_ndarray - target_pos_ndarray))\n",
    "        dists.append(tmp)\n",
    "\n",
    "    dists_ndarray = np.array(dists)\n",
    "    mst_r = minimum_spanning_tree(dists_ndarray)\n",
    "\n",
    "    new_nx_graph = nx.Graph()\n",
    "    nodes = [node_id for node_id in pos]\n",
    "    new_nx_graph.add_nodes_from(nodes)\n",
    "    weighted_edges = []\n",
    "    for mst_v in list(zip(*mst_r.nonzero())):\n",
    "        source_id = index_id_map[mst_v[0]]\n",
    "        target_id = index_id_map[mst_v[1]]\n",
    "        weighted_edges.append((source_id, target_id, EDGE_WEIGHT))\n",
    "    new_nx_graph.add_weighted_edges_from(weighted_edges)\n",
    "\n",
    "    return new_nx_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 類似度\n",
    "\n",
    "def jaccard_similarity_sum(nx_graph, nx_shape_graph):\n",
    "    j_s_sum = 0\n",
    "    for node in nx_graph.nodes:\n",
    "        g_n = [n for n in nx_graph.neighbors(node)]\n",
    "        s_n = [n for n in nx_shape_graph.neighbors(node)]\n",
    "        and_n = list(set(g_n) & set(s_n))\n",
    "        or_n = list(set(g_n + s_n))\n",
    "\n",
    "        j_s_sum += len(and_n) / len(or_n)\n",
    "\n",
    "    return j_s_sum / len(nx_graph.nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ideal_edge_length(nx_graph, pos):\n",
    "    s = 0\n",
    "    for source, target, data in nx_graph.edges(data=True):\n",
    "        weight = data['weight'] if 'weight' in data else 1\n",
    "        shortest_path_length = nx.shortest_path_length(\n",
    "            nx_graph, source, target, data['weight'] if 'weight' in data else None)\n",
    "\n",
    "        lij = shortest_path_length * weight\n",
    "        dist = np.linalg.norm(np.array(pos[source]) - np.array(pos[target]))\n",
    "        s += ((dist - lij) / lij) ** 2\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_edge_crossing(p1, p2, p3, p4):\n",
    "    tc1 = (p1[0] - p2[0]) * (p3[1] - p1[1]) + (p1[1] - p2[1]) * (p1[0] - p3[0])\n",
    "    tc2 = (p1[0] - p2[0]) * (p4[1] - p1[1]) + (p1[1] - p2[1]) * (p1[0] - p4[0])\n",
    "    td1 = (p3[0] - p4[0]) * (p1[1] - p3[1]) + (p3[1] - p4[1]) * (p3[0] - p1[0])\n",
    "    td2 = (p3[0] - p4[0]) * (p2[1] - p3[1]) + (p3[1] - p4[1]) * (p3[0] - p2[0])\n",
    "    return tc1 * tc2 < 0 and td1 * td2 < 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_crossing_finder(nx_graph, pos):\n",
    "    edges = {}\n",
    "    for s1, t1, attr1 in nx_graph.edges(data=True):\n",
    "        id1 = attr1['id']\n",
    "        if id1 not in edges:\n",
    "            edges[id1] = {}\n",
    "        for s2, t2, attr2 in nx_graph.edges(data=True):\n",
    "            id2 = attr2['id']\n",
    "            crossing = is_edge_crossing(pos[s1], pos[t1], pos[s2], pos[t2])\n",
    "            edges[id1][id2] = crossing\n",
    "\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossing_number(nx_graph, pos, edge_crossing=None):\n",
    "    if edge_crossing is None:\n",
    "        edge_crossing = edge_crossing_finder(nx_graph, pos)\n",
    "\n",
    "    s = 0\n",
    "    for i in edge_crossing:\n",
    "        if edge_crossing[i]:\n",
    "            s += 1\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossing_angle_maximization(nx_graph, pos, edge_crossing=None):\n",
    "    if edge_crossing is None:\n",
    "        edge_crossing = edge_crossing_finder(nx_graph, pos)\n",
    "    s = 0\n",
    "    for s1, t1, attr1 in nx_graph.edges(data=True):\n",
    "        id1 = attr1['id']\n",
    "        for s2, t2, attr2 in nx_graph.edges(data=True):\n",
    "            id2 = attr2['id']\n",
    "            if edge_crossing[id1][id2] or edge_crossing[id2][id1]:\n",
    "                e1 = np.array(pos[s1]) - np.array(pos[t1])\n",
    "                e2 = np.array(pos[s2]) - np.array(pos[t2])\n",
    "                s += np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2))\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravity_center(pos):\n",
    "    gx = 0\n",
    "    gy = 0\n",
    "    for node_id in pos:\n",
    "        x, y = pos[node_id]\n",
    "        gx += x\n",
    "        gy += y\n",
    "\n",
    "    gx /= len(pos)\n",
    "    gy /= len(pos)\n",
    "\n",
    "    return gx, gy\n",
    "\n",
    "\n",
    "def aspect_ratio(pos):\n",
    "    gravity_centered_pos = []\n",
    "    gx, gy = gravity_center(pos)\n",
    "    for node_id in pos:\n",
    "        x, y = pos[node_id]\n",
    "        gravity_centered_pos.append([x - gx, y - gy])\n",
    "    gravity_centered_pos = np.array(gravity_centered_pos)\n",
    "    _, s, _ = np.linalg.svd(gravity_centered_pos, full_matrices=True)\n",
    "    a = max(s)\n",
    "    b = min(s)\n",
    "\n",
    "    return b / a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_degree(nx_graph):\n",
    "    md = 0\n",
    "    for node_id in nx_graph.nodes:\n",
    "        d = nx_graph.degree[node_id]\n",
    "        if md < d:\n",
    "            md = d\n",
    "\n",
    "    return md\n",
    "\n",
    "# すべてのノードについて、あるノードに入射するエッジ同士のなす角度が最も小さいもの\n",
    "def angular_resolution(nx_graph, pos):\n",
    "    edges = {}\n",
    "    for s, t in nx_graph.edges:\n",
    "        if s not in edges:\n",
    "            edges[s] = {}\n",
    "        if t not in edges:\n",
    "            edges[t] = {}\n",
    "        edges[s][t] = True\n",
    "        edges[t][s] = True\n",
    "\n",
    "    ls = 0\n",
    "    for s in pos:\n",
    "        if s not in edges:\n",
    "            continue\n",
    "        l = 2 * math.pi\n",
    "        ts = sorted([node_id for node_id in edges[s]])\n",
    "        for i, t1 in enumerate(ts):\n",
    "            e1 = np.array(pos[s]) - np.array(pos[t1])\n",
    "            for t2 in ts[i + 1:]:\n",
    "                if t1 == t2 or s == t1 or s == t2:\n",
    "                    continue\n",
    "                e2 = np.array(pos[s]) - np.array(pos[t2])\n",
    "                angle = math.acos(\n",
    "                    np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2)))\n",
    "                if angle < l:\n",
    "                    l = angle\n",
    "        ls += l\n",
    "    return ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノードのユークリッド距離として定義される。stressなどと一致させるために正規化するらしいけど多分必要ない\n",
    "def node_resolution(pos):\n",
    "    nodes = sorted([node_id for node_id in pos])\n",
    "    r = 0\n",
    "    for i, sid in enumerate(nodes):\n",
    "        for tid in nodes[i + 1:]:\n",
    "            dist = np.linalg.norm(np.array(pos[sid]) - np.array(pos[tid]))\n",
    "            if dist < r:\n",
    "                r = dist\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エッジを直径とした円の内部にノードを含まないようにしたい。\n",
    "# そこで内部に含まれる点\n",
    "def gabriel_graph_property(nx_graph,pos):\n",
    "    s = 0\n",
    "    for e in nx_graph.edges:\n",
    "        e1, e2 = e\n",
    "        x1 = np.array(pos[e1])\n",
    "        x2 = np.array(pos[e2])\n",
    "        rij = np.linalg.norm(x1 - x2) / 2\n",
    "        cij = (np.array(x1) + np.array(x2)) / 2\n",
    "        for node_id in nx_graph.nodes:\n",
    "            if e1 == node_id or e2 == node_id:\n",
    "                continue\n",
    "            s += max(0, rij - np.linalg.norm(np.array(pos[node_id] - cij)))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフの描画\n",
    "\n",
    "def draw_graph(graph, indices):\n",
    "    drawing = Coordinates.initial_placement(graph)\n",
    "    rng = Rng.seed_from(0)  # random seed\n",
    "    sgd = SparseSgd(\n",
    "        graph,\n",
    "        lambda _: 30,  # edge length\n",
    "        50,  # number of pivots\n",
    "        rng,\n",
    "    )\n",
    "    scheduler = sgd.scheduler(\n",
    "        100,  # number of iterations\n",
    "        0.1,  # eps: eta_min = eps * min d[i, j] ^ 2\n",
    "    )\n",
    "\n",
    "    def step(eta):\n",
    "        sgd.shuffle(rng)\n",
    "        sgd.apply(drawing, eta)\n",
    "\n",
    "    scheduler.run(step)\n",
    "\n",
    "    pos = {u: (drawing.x(i), drawing.y(i)) for u, i in indices.items()}\n",
    "\n",
    "    return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_based_metrics_values = {}\n",
    "positions = {}\n",
    "\n",
    "for nx_graph, data, filename in zip(nx_graphs, graph_data, DATASET_NAMES):\n",
    "    graph, indices = data\n",
    "    pos = draw_graph(graph, indices)\n",
    "\n",
    "    graph_degrees = None\n",
    "    distances = None\n",
    "\n",
    "    shape_based_metrics_values[filename] = {}\n",
    "    positions[filename] = pos\n",
    "\n",
    "    # shape graph by delaunay triangulation\n",
    "    nx_shape_graph = generate_delaunay_triangulation_graph(pos)\n",
    "    shape_based_metrics_values[filename][f'd-t'] = jaccard_similarity_sum(\n",
    "        nx_graph, nx_shape_graph)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5031a1c082d374b42824ba4074361c835f9bfd3c33854874e61fd39ef2ddb484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

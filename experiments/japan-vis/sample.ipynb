{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_WEIGHT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import argparse\n",
    "from statistics import median\n",
    "import json\n",
    "\n",
    "# Third Party Library\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from egraph import Drawing, all_sources_bfs\n",
    "from ex_utils.config.dataset import dataset_names\n",
    "from ex_utils.config.paths import get_dataset_path\n",
    "from ex_utils.config.quality_metrics import qm_names\n",
    "from ex_utils.share import (\n",
    "    draw_and_measure,\n",
    "    ex_path,\n",
    "    generate_seed_median_df,\n",
    "    generate_sscalers,\n",
    "    pivots2rate,\n",
    ")\n",
    "from ex_utils.utils.graph import (\n",
    "    egraph_graph,\n",
    "    load_nx_graph,\n",
    "    nx_graph_preprocessing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ex_path.joinpath(\"data/random.json\")) as f:\n",
    "    random_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = list(range(15))\n",
    "n_split = 10\n",
    "n_samples = [50, 100, 500]\n",
    "d_name = \"USpowerGrid\"\n",
    "\n",
    "for random_pref in random_data:\n",
    "    pref = {}\n",
    "    pref_sum = sum(random_pref)\n",
    "    for qm_name, p in zip(qm_names, random_pref):\n",
    "        pref[qm_name] = p / pref_sum\n",
    "\n",
    "    print(pref)\n",
    "\n",
    "    baseline_df_paths = [\n",
    "        ex_path.joinpath(\n",
    "            f\"data/grid/{d_name}/n_split={n_split}/seed={data_seed}.pkl\"\n",
    "        )\n",
    "        for data_seed in seeds\n",
    "    ]\n",
    "    baseline_df = generate_seed_median_df(\n",
    "        pd.concat([pd.read_pickle(df_path) for df_path in baseline_df_paths])\n",
    "    )\n",
    "    baseline_sscalers = generate_sscalers(baseline_df)\n",
    "\n",
    "    baseline_df[f\"sscaled_{qm_name}\"] = baseline_sscalers[qm_name].transform(\n",
    "        baseline_df[f\"values_{qm_name}\"].values.reshape(-1, 1)\n",
    "    )\n",
    "\n",
    "    baseline_df[\"weighted_sscaled_sum\"] = sum(\n",
    "        [\n",
    "            target_df[f\"sscaled_{qm_name}\"] * pref[qm_name]\n",
    "            for qm_name in qm_names\n",
    "        ]\n",
    "    )\n",
    "    baseline_max = baseline_df.loc[\n",
    "        baseline_df[\"weighted_sscaled_sum\"].idxmax()\n",
    "    ]\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for n_sample in n_samples:\n",
    "        result[\"all\"] = 0\n",
    "        result[n_sample] = 0\n",
    "        points_dir = ex_path.joinpath(\n",
    "            f\"data/sampled_points/{d_name}/n_split={n_split}/n_sample={n_sample}/\"\n",
    "        )\n",
    "        for path in points_dir.iterdir():\n",
    "            result[\"all\"] += 1\n",
    "            target_df = pd.read_pickle(path)\n",
    "            sscalers = generate_sscalers(target_df)\n",
    "\n",
    "            for qm_name in qm_names:\n",
    "                target_df[f\"sscaled_{qm_name}\"] = sscalers[qm_name].transform(\n",
    "                    target_df[f\"values_{qm_name}\"].values.reshape(-1, 1)\n",
    "                )\n",
    "\n",
    "            target_df[\"weighted_sscaled_sum\"] = sum(\n",
    "                [\n",
    "                    target_df[f\"sscaled_{qm_name}\"] * pref[qm_name]\n",
    "                    for qm_name in qm_names\n",
    "                ]\n",
    "            )\n",
    "            max_row = target_df.loc[target_df[\"weighted_sscaled_sum\"].idxmax()]\n",
    "\n",
    "            scaled_qm = dict(\n",
    "                [\n",
    "                    (\n",
    "                        qm_name,\n",
    "                        baseline_sscalers[qm_name].transform(\n",
    "                            [[max_row[f\"values_{qm_name}\"]]]\n",
    "                        )[0][0],\n",
    "                    )\n",
    "                    for qm_name in qm_names\n",
    "                ]\n",
    "            )\n",
    "            weighted_sacled_qm_sum = sum(\n",
    "                [scaled_qm[qm_name] * pref[qm_name] for qm_name in qm_names]\n",
    "            )\n",
    "\n",
    "            if baseline_max[\"weighted_sscaled_sum\"] < weighted_sacled_qm_sum:\n",
    "                result[n_sample] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(\n",
    "    \"/Users/fuga_takata/dev/vdslab-project/hyperparameter-in-graph-drawing/experiments/japan-vis/data/sampled_points/1138_bus/n_split=10/n_sample=50/0aeab76d-55a8-48a3-9978-f0b1017f6c0c.pkl\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
